{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from os import listdir, path\n",
    "from sudachipy import tokenizer, dictionary\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SudachiAnalizer():\n",
    "    \n",
    "    def get_token(self, source) :\n",
    "        \n",
    "        tokenizer_obj = dictionary.Dictionary().create()\n",
    "\n",
    "        mode = tokenizer.Tokenizer.SplitMode.C\n",
    "        result = tokenizer_obj.tokenize(source, mode)\n",
    "\n",
    "        word_list = []\n",
    "        for mrph in result:\n",
    "            if not (mrph == \"\"):\n",
    "                norm_word = mrph.normalized_form()\n",
    "                hinsi = mrph.part_of_speech()[0] \n",
    "\n",
    "                # 単語の正規表現が特定の品詞の場合のみ採用する\n",
    "                if hinsi in  [\"名詞\", \"動詞\", \"形容詞\"]:\n",
    "                    word = tokenizer_obj.tokenize(norm_word, mode)[0].dictionary_form()\n",
    "                    word_list.append(word)\n",
    "\n",
    "        return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "PATH = \"text/\"\n",
    "\n",
    "sudachi = SudachiAnalizer()\n",
    "\n",
    "#  pathの中のdir(txt以外)をlistにして返す\n",
    "def corpus_subdirs(path):\n",
    "    subdirs = []\n",
    "    for x in listdir(path):\n",
    "        if not x.endswith('.txt'):\n",
    "            subdirs.append(x)\n",
    "    return subdirs\n",
    "\n",
    "# pathの中のファイルをlistにして返す\n",
    "def corpus_filenames(path):\n",
    "    labels = [] # *.txt\n",
    "    for y in listdir(path):\n",
    "        if not y.startswith('LICENSE'):\n",
    "            labels.append(y)\n",
    "    return labels\n",
    "\n",
    "for dir in corpus_subdirs(PATH):\n",
    "    for file in corpus_filenames(PATH+dir):\n",
    "        corpus_data = open(path.join(PATH + dir + \"/\" + file), \"r\")\n",
    "        source = corpus_data.read()\n",
    "        token = sudachi.get_token(source)\n",
    "        corpus_data = {\"name\" : file, \"tag\" : dir, \"token\" : token}\n",
    "        docs.append(corpus_data)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model/corpus.pkl', 'wb') as temp:\n",
    "  pickle.dump(docs, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('model/corpus.pkl','rb')\n",
    "docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書の数の確認\n",
    "tag_list = []\n",
    "for doc in docs:\n",
    "    tag_list.append(doc[\"tag\"])\n",
    "    \n",
    "df = pd.DataFrame(tag_list)\n",
    "tag_counts = df[0].value_counts()\n",
    "tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docsの走査の順番を確認\n",
    "tag = \"\"\n",
    "for i in tag_list:\n",
    "    if i != tag :\n",
    "        print(i)\n",
    "        tag = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "\n",
    "for item in docs:\n",
    "    text_list.append(item[\"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary =corpora.Dictionary(text_list)\n",
    "dictionary.filter_extremes(no_below=2,no_above=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[dictionary.doc2bow(tokens) for tokens in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トピック数の設定\n",
    "num_topics=20\n",
    "\n",
    "#モデルの学習\n",
    "model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=5)\n",
    "model.save('model/lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediaごとのトピック分布の確認\n",
    "prob_doc = np.array(model.get_document_topics(corpus, minimum_probability=0))[:,:,1]\n",
    "\n",
    "# DataFrameに収納\n",
    "L=[ z for z in range(1,num_topics+1)]\n",
    "col_name = list(map(lambda x: \"Prob_\"+str(x),L))\n",
    "df_prob = pd.DataFrame(prob_doc)\n",
    "df_prob.columns = col_name\n",
    "\n",
    "\n",
    "def del_Prob(x):\n",
    "    return int(x.split(\"_\")[1])\n",
    "\n",
    "df_prob[\"Max\"] = df_prob.idxmax(axis=1)\n",
    "df_prob[\"Max\"] = df_prob[\"Max\"].apply(lambda x : del_Prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "for count in tag_counts:\n",
    "    \n",
    "    df_topic = pd.DataFrame(df_prob[start:start+count].drop(\"Max\",axis=1).sum()/df_prob[start:start+count].drop(\"Max\",axis=1).sum().sum())\n",
    "    df_topic.columns = [\"Prob\"]\n",
    "    df_topic[\"Topic\"] = [ z for z in range(1,num_topics+1)]    \n",
    "\n",
    "    plt.figure(figsize = (10,6))\n",
    "    ax = sns.barplot(x=\"Topic\",y=\"Prob\",data=df_topic,color=\"darkblue\")\n",
    "    ax.set_xlabel(\"Topic\",fontsize=10)\n",
    "    ax.set_ylabel(\"Prob\",fontsize=10)\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "    plt.title(tag_list[start])\n",
    "    plt.show()\n",
    "\n",
    "    start += count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pcoa = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "\n",
    "# save as html\n",
    "pyLDAvis.display(vis_pcoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import CoherenceModel\n",
    "\n",
    "array = []\n",
    "for i in range(1, 31):\n",
    "\n",
    "    lda = models.LdaModel(corpus = corpus, id2word = dictionary, num_topics =i, random_state = 5)\n",
    "\n",
    "    cm = CoherenceModel(model = lda, corpus = corpus, coherence = 'u_mass')\n",
    "    coherence = cm.get_coherence()\n",
    "\n",
    "    perwordbound = lda.log_perplexity(corpus)\n",
    "    perplexity = np.exp2(-perwordbound)\n",
    "    \n",
    "    array.append([i, coherence, perplexity])\n",
    "    \n",
    "    print(f\"num_topics = {i}, coherence = {coherence}, perplexity = {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame(array)\n",
    "evaluation_ =  evaluation[[1, 2]]\n",
    "evaluation_.columns = [\"coherence\", \"perplexity\"]\n",
    "\n",
    "ax = evaluation_.plot(secondary_y=[\"perplexity\"], figsize=(16,4), alpha=0.5, legend=True)\n",
    "ax.set_ylabel('coherence ', fontsize=10)\n",
    "ax.right_ax.set_ylabel('perplexity ', fontsize=10)\n",
    "ax.set_xlabel('num_topics', fontsize=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#トピック数の設定\n",
    "num_topics=9\n",
    "\n",
    "#モデルの学習\n",
    "model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=5)\n",
    "model.save('model/lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediaごとのトピック分布の確認\n",
    "prob_doc = np.array(model.get_document_topics(corpus, minimum_probability=0))[:,:,1]\n",
    "\n",
    "# DataFrameに収納\n",
    "L=[ z for z in range(1,num_topics+1)]\n",
    "col_name = list(map(lambda x: \"Prob_\"+str(x),L))\n",
    "df_prob = pd.DataFrame(prob_doc)\n",
    "df_prob.columns = col_name\n",
    "\n",
    "\n",
    "def del_Prob(x):\n",
    "    return int(x.split(\"_\")[1])\n",
    "\n",
    "df_prob[\"Max\"] = df_prob.idxmax(axis=1)\n",
    "df_prob[\"Max\"] = df_prob[\"Max\"].apply(lambda x : del_Prob(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "\n",
    "for count in tag_counts:\n",
    "    \n",
    "    df_topic = pd.DataFrame(df_prob[start:start+count].drop(\"Max\",axis=1).sum()/df_prob[start:start+count].drop(\"Max\",axis=1).sum().sum())\n",
    "    df_topic.columns = [\"Prob\"]\n",
    "    df_topic[\"Topic\"] = [ z for z in range(1,num_topics+1)]    \n",
    "\n",
    "    plt.figure(figsize = (10,6))\n",
    "    ax = sns.barplot(x=\"Topic\",y=\"Prob\",data=df_topic,color=\"darkblue\")\n",
    "    ax.set_xlabel(\"Topic\",fontsize=10)\n",
    "    ax.set_ylabel(\"Prob\",fontsize=10)\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "    plt.title(tag_list[start])\n",
    "    plt.show()\n",
    "\n",
    "    start += count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_pcoa = pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)\n",
    "\n",
    "# save as html\n",
    "pyLDAvis.display(vis_pcoa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
